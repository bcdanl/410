---
title: Lecture 2
subtitle: Omitted Variable Bias; Bad Controls
format:
  clean-revealjs:
    self-contained: false
    chalkboard: true
    incremental: false
    code-annotations: hover
    scrollable: false

    # logo: logo-title-slide.png
author:
  - name: Byeong-Hak Choe
    email: bchoe@geneseo.edu
    affiliations: SUNY Geneseo
date: 2026-01-26
execute: 
  eval: true
  echo: false
callout-icon: false

from: markdown+emoji
include-after-body: target-hover.html # effect.html

# bibliography: refs.bib
---

```{r setup}
#| include: false

library(tidyverse)
library(lubridate)
library(skimr)
library(ggthemes)
library(hrbrthemes)
library(viridis)
library(rmarkdown)
library(gapminder)
library(ggrepel)

theme_set(theme_fivethirtyeight() +
            theme(strip.background =element_rect(fill="lightgray"),
                axis.title.x = 
                  element_text(angle = 0,
                               size = rel(1.75),
                               margin = margin(10,0,0,0)),
                axis.title.y = 
                  element_text(angle = 0,
                               size = rel(1.75),
                               margin = margin(0,10,0,0)),
                axis.text.x = element_text(size = rel(1.75)),
                axis.text.y = element_text(size = rel(1.75)),
                strip.text = element_text(size = rel(1.5)),
                legend.position = "top",
                legend.text = element_text(size = rel(1.5)),
                legend.title = element_text(size = rel(1.5))
                )
          )

# Set global options for color-blind-friendly scales
# scale_colour_discrete <- function(...) scale_colour_viridis_d(...)
scale_colour_discrete <- function(...) scale_color_colorblind(...)
scale_fill_discrete <- function(...) scale_fill_colorblind(...)
```



# âŒ **Omitted Variable Bias** {background-color="#1c4982"}


## Omitted Variable Bias

- **Omitted variable bias (OVB)**: bias in the model because of omitting an important regressor that is
correlated with existing regressor(s).


- Let's use an orange juice (OJ) example to demonstrate the OVB.
  - OJ price elasticity estimates vary with models, whether or not taking into account `brand` or `ad_status`

Variable  | Description
-|-
`sales`   | Quantity of OJ cartons sold
`price`   | Price of OJ
`brand`   | Brand of OJ
`ad`    | Advertisement status



## ğŸ“ Short- and Long- Regressions
- OVB is the difference in beta estimates between short- and long-form regressions.

- **Short regression**: The regression model with less regressors

<div style="display:block; margin:-25px;"></div>

$$
\begin{align}
\log(\text{sales}_i) = \beta_0 + \beta_1\log(\text{price}_i) + \epsilon_i
\end{align}
$$

- **Long regression**: The regression model that adds additional regressor(s) to the short one.

<div style="display:block; margin:-25px;"></div>

$$
\begin{align}
\log(\text{sales}_i) =& \beta_0 + \beta_{1}\log(\text{price}_i) \\
&+ \beta_{2}\text{minute.maid}_i + \beta_{3}\text{tropicana}_i + \epsilon_i
\end{align}
$$

- OVB for $\beta_1$ is:

<div style="display:block; margin:-40px;"></div>

$$
\text{OVB} = \widehat{\beta_{1}^{short}} - \widehat{\beta_{1}^{long}}
$$

## OVB formula

- Consider the following short- and long- regressions:
  - Short: $Y_i = \beta_0 + \beta_{1}^{short}X_1 + \epsilon_{short}$  
  - Long: $Y_i = \beta_0 + \beta_{1}^{long}X_1 +\beta_{2}X_2 + \epsilon_{long}$


- Error in short form can be represented as:
$$
{\epsilon_{short}} = \beta_{2}X_2 + \epsilon_{long}
$$

- If variable $X_1$ is **correlated** with $X_2$, the following assumptions are violated in the <u>short regression</u> model:
  - Errors are not correlated with regressors.
  - Errors have a mean value of 0.




## â“ğŸ” How does an OVB happen in regression?

- In the first stage, consider the relationship between `price` and `brand`:

$$
\log(\text{price}) = \beta_0 + \beta_1\text{minute_maid} + \beta_2\text{tropicana} + \epsilon_{1st}
$$


- Then, calculate the residual:
$$
\widehat{\epsilon_{1st}} = \log(\text{price}) - \widehat{\log(\text{price})}
$$
- The residual represents the log of OJ price after its correlation with brand has been removed!

- In the second stage, regress $\log(\text{sales})$ on residual $\widehat{\epsilon_{1st}}$:

$$
\log(\text{sales}) = \beta_0 + \beta_1\widehat{\epsilon_{1st}}  + \epsilon_{2nd}
$$


## Regression Sensitivity Analysis
- Regression finds the coefficients on the part of each regressor that is **independent** from the other regressors.

- What can we do to deal with OVB problems?
  - Because we can never be sure whether a given set of
controls is enough to eliminate OVB, itâ€™s important to ask how sensitive regression results are to changes in the list of controls.


# ğŸš« **Bad Controls**  {background-color="#1c4982"}

## ğŸš« Bad Controls: When â€œControlling Moreâ€ Makes Things Worse

A common mistake in regression is thinking:

> âœ… *"If I add more controls, the model must get better."*

But some controls are **bad controls** â€” they can actually **bias** your estimate.


## ğŸš« What is a "Bad Control"?

A **bad control** is a variable that:

- is **influenced by** the explanatory variable $x$, or
- is part of the **process that links** $x$ to the outcome $y$

Including a bad control can â€œcontrol awayâ€ the variation in $y$ that is connected to $x$,
making the estimated relationship look smaller than it really is.


## ğŸ§  Intuition: What Should a "Good Control" Look Like?

A **good control** is a variable that:

âœ… helps explain variation in the outcome $y$  
âœ… is related to the explanatory variable $x$  
âœ… is **not a consequence of $x$** (it exists â€œin the backgroundâ€)

Thatâ€™s the key difference.



## ğŸ”— Example 1: Education and Wages ğŸ’¼

Suppose we want to estimate:

> Does education increase wages?

### âœ… Good regression idea:
$$
wage_i = \beta_0 + \beta_1 education_i + u_i
$$

Maybe add good controls like:

- age
- gender
- region
- parent education (if available)



## ğŸš« Bad Control: Occupation or Job Type

But what if we control for **occupation**?

$$
wage_i = \beta_0 + \beta_1 education_i + \beta_2 occupation_i + u_i
$$

Why is that bad?

::: fragment
Because education affects occupation:

$$
education \rightarrow occupation \rightarrow wage
$$

So controlling for occupation:

ğŸš« blocks part of the education effect  
â†’ we **underestimate** the impact of education

:::

## ğŸŒ± Example 2: Fertilizer, Plant Height, and Crop Yield


:::: {.columns}

::: {.column width="50%"}

We want to estimate:

> **Does fertilizer increase crop yield?**

But `fertilizer` changes the plantâ€™s growth:

$$
fertilizer \rightarrow height
$$

And `height` is strongly related to yield:

$$
height \rightarrow yield
$$

:::

::: {.column width="50%"}

Also, some plots are simply better than others because of `soil quality`:

$$
\begin{align}
&soil\ quality \rightarrow height\\
&soil\ quality \rightarrow yield
\end{align}
$$

So **soil quality** is a hidden factor that affects both height and yield.

:::
::::

## ğŸš« Bad Control: Plant Height

But what if we control for **plant height**?

$$
yield_i = \beta_0 + \beta_1 fertilizer_i + \beta_2 height_i + u_i
$$

Why is that bad?

::: fragment
Because fertilizer affects plant height:

$$
fertilizer \rightarrow height \rightarrow yield
$$

So controlling for height:

ğŸš« blocks an important pathway through which fertilizer raises yield  
â†’ we **underestimate** the impact of fertilizer

:::

## âœ… Good Controls vs. Bad Controls (Big Picture)

| Type | Example | Why it matters |
|---|---|---|
| âœ… Good control | Age, gender, region, baseline ability | Helps reduce omitted variable bias by holding constant **pre-existing differences** |
| ğŸš« Bad control | Occupation (when studying education â†’ wages) | Can remove part of the education effect by controlling for something education influences |
| ğŸš« Bad control | Plant height (when studying fertilizer â†’ yield) | Can remove part of the fertilizer effect by controlling for something fertilizer influences |



## ğŸ¯ Why Bad Controls Are Dangerous

Bad controls can cause two problems:

:::: {.columns}

::: {.column width="50%"}

### 1) âŒ Block part of the true effect
You unintentionally remove the pathway:

$$
x \rightarrow control \rightarrow y
$$

So the estimated $\hat{\beta}_1$ becomes **too small**.

:::
::: {.column width="50%"}

### 2) âŒ Create new bias 

Sometimes the control is influenced by **both** $x$ and $y$:

$$
x \rightarrow control \leftarrow y
$$

Then controlling for it creates a **fake association** between $x$ and $y$  
even if none existed before.

:::
::::



## âœ… Quick Rule: Should We Control for It?

Before adding a variable $z$ as a control, ask:

- **Does $z$ exist *before* $x$ is determined?**  
  âœ… If yes â†’ usually safe to control for

<div style="display:block; margin:25px;"></div>


- **Does $z$ change *after* $x$ changes?**  
  ğŸš« If yes â†’ it may be a **bad control**

<div style="display:block; margin:25px;"></div>

- **Is $z$ a pre-existing characteristic that is related to both $x$ and $y$?**  
  âœ… If yes â†’ it is often a **useful control**



## ğŸ§¾ Summary: "More Controls" â‰  "Better"

- âœ… Controls are helpful when they fix omitted variable bias  
- ğŸš« Controls are harmful when they â€œcontrol awayâ€ the effect you want

<div style="display:block; margin:25px;"></div>


- **The goal is not to control for everything.**  
- The goal is to control for the **right things**.



